{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Crawler\n",
    "\n",
    "## 1.0. Related example\n",
    "\n",
    "This code shows `wget`-like tool written in python. Run it from console (`python wget.py`), make it work. Check the code, reuse, and modify for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "\n",
    "def wget(url, filename):\n",
    "    # allow redirects - in case file is relocated\n",
    "    resp = requests.get(url, allow_redirects=True)\n",
    "    # this can also be 2xx, but for simplicity now we stick to 200\n",
    "    # you can also check for `resp.ok`\n",
    "    if resp.status_code != 200:\n",
    "        print(resp.status_code, resp.reason, 'for', url)\n",
    "        return\n",
    "    \n",
    "    # just to be cool and print something\n",
    "    print(*[f\"{key}: {value}\" for key, value in resp.headers.items()], sep='\\n')\n",
    "    print()\n",
    "    \n",
    "    # try to extract filename from url\n",
    "    if filename is None:\n",
    "        # start with http*, ends if ? or # appears (or none of)\n",
    "        m = re.search(\"^http.*/([^/\\?#]*)[\\?#]?\", url)\n",
    "        filename = m.group(1)\n",
    "        if not filename:\n",
    "            raise NameError(f\"Filename neither given, nor found for {url}\")\n",
    "\n",
    "    # what will you do in case 2 websites store file with the same name?\n",
    "    if os.path.exists(filename):\n",
    "        raise OSError(f\"File {filename} already exists\")\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(resp.content)\n",
    "        print(f\"File saved as {filename}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description='download file.')\n",
    "#     parser.add_argument(\"-O\", type=str, default=None, dest='filename', help=\"output file name. Default -- taken from resource\")\n",
    "#     parser.add_argument(\"url\", type=str, default=None, help=\"Provide URL here\")\n",
    "#     args = parser.parse_args()\n",
    "#     wget(args.url, args.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0.1. How to parse a page?\n",
    "\n",
    "If you build a crawler, you might follow one of the approaches:\n",
    "1. search for URLs in the page, assuming this is just a text.\n",
    "2. search for URLs in the places where URLs should appear: `<a href=..`, `<img src=...`, `<iframe src=...` and so on.\n",
    "\n",
    "To follow the first approach you can rely on some good regular expression. [Like this](https://stackoverflow.com/a/3809435).\n",
    "\n",
    "To follow the second approach just read one of these: [short answer](https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup) or [exhaustive explanation](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. [15] Download and persist #\n",
    "Please complete a code for `load()`, `download()` and `persist()` methods of `Document` class. What they do:\n",
    "- for a given URL `download()` method downloads binary data and stores in `self.content`. It returns `True` for success, else `False`.\n",
    "- `persist()` method saves `self.content` somewhere in file system. We do it to avoid multiple downloads (for caching in other words).\n",
    "- `load()` method loads data from hard drive. Returns `True` for success.\n",
    "\n",
    "Tests checks that your code somehow works.\n",
    "\n",
    "**NB Passing the test doesn't mean you correctly completed the task.** These are **criteria, which have to be fullfilled**:\n",
    "1. URL is a unique identifier (as it is a subset of URI). Thus, documents with different URLs should be stored in different files. Typical errors: documents from the same domain are overwritten to the same file, URLs with similar endings are downloaded to the same file, etc.\n",
    "2. The document can be not only a text file, but also a binary. Pay attention that if you download `mp3` file, it still can be played. Hint: don't hurry to convert everything to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "\n",
    "class Document:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.dir = './files'\n",
    "        self.create_directory()\n",
    "        self.generate_path()\n",
    "\n",
    "    def create_directory(self):\n",
    "        if not os.path.exists(self.dir):\n",
    "            os.makedirs(self.dir)\n",
    "\n",
    "    def generate_path(self):\n",
    "        m = re.search(\".*([.]\\w*)[\\?#]?\", self.url)\n",
    "        file_extension = m.group(1)\n",
    "\n",
    "        filename = quote(self.url)\n",
    "        self.filename = hashlib.md5(filename.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        if file_extension in ['.pdf', '.mp3', '.avi', '.mp4', '.txt', '.png', '.jpg', '.html']:\n",
    "            self.filename += file_extension\n",
    "\n",
    "        self.path = os.path.join(self.dir, self.filename)\n",
    "        \n",
    "    def get(self):\n",
    "        if not self.load():\n",
    "            if not self.download():\n",
    "                raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "    \n",
    "    def download(self):\n",
    "        '''Downloads self.url content, stores it in self.content and returns True in case of success.\n",
    "        '''\n",
    "        try:\n",
    "            resp = requests.get(self.url, allow_redirects=True, timeout=10)\n",
    "            if resp.status_code != 200:\n",
    "                print(resp.status_code, resp.reason, 'for', self.url)\n",
    "                return False\n",
    "            \n",
    "            self.content = resp.content\n",
    "            return True\n",
    "        \n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def persist(self):\n",
    "        '''Writes document content to hard drive.\n",
    "        '''\n",
    "        with open(self.path, 'wb') as f:\n",
    "            f.write(self.content)\n",
    "            # print(f\"File saved in {self.dir} as {self.filename}\")\n",
    "            \n",
    "    def load(self):\n",
    "        '''Loads content from hard drive, stores it in self.content and returns True in case of success.\n",
    "        '''\n",
    "        try:\n",
    "            with open(self.path, 'rb') as f:\n",
    "                self.content = f.read()\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = Document('https://i.pinimg.com/originals/9c/cf/9e/9ccf9ed7a40f29bef23dc20bf5fe13b5.jpg')\n",
    "pic.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot = Document('http://sprotasov.ru/data/iu.txt')\n",
    "prot.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document('http://sprotasov.ru/data/iu.txt')\n",
    "\n",
    "doc.get()\n",
    "assert doc.content, \"Document download failed\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
    "\n",
    "doc.get()\n",
    "assert doc.load(), \"Load should return true for saved document\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. [10] Parse HTML\n",
    "`BeautifulSoap` library is a de facto standard to parse XML and HTML documents in python. Use it to complete `parse()` method that extracts document contents. You should initialize:\n",
    "1. `self.anchors` list of tuples `('text', 'url')` met in a document. Be aware, there exist relative links (e.g. `../content/pic.jpg`). Use `urllib.parse.urljoin()` to fix this issue.\n",
    "2. `self.images` list of images met in a document. Again, links can be relative to current page.\n",
    "3. `self.text` should keep plain text of the document without scripts, tags, comments and so on. You can refer to [this stackoverflow answer](https://stackoverflow.com/a/1983219) for details.\n",
    "\n",
    "**NB All these 3 criteria must be fulfilled to get full point for the task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "\n",
    "    def get_anchors_from_html(self):\n",
    "        all_hrefs = self.soup.find_all('a', href=True)\n",
    "        anchors = set()\n",
    "\n",
    "        for a in all_hrefs:\n",
    "            url = urljoin(self.url, a[\"href\"])\n",
    "            if re.match(\"^http.*/[^/\\?#]*[\\?#]?\", url):\n",
    "                anchors.add((a.get_text(), url))\n",
    "\n",
    "        anchors = list(anchors)\n",
    "        return anchors\n",
    "    \n",
    "    def get_images_from_html(self):\n",
    "        urls = []\n",
    "\n",
    "        for image_url in self.soup.find_all('img'):\n",
    "            try:\n",
    "                src = image_url['src']\n",
    "                src = urljoin(self.url, src)\n",
    "                urls.append(src)\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        urls = list(set(urls))\n",
    "        return urls\n",
    "\n",
    "    def tag_visible(self, element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def get_text_from_html(self):\n",
    "        texts = self.soup.findAll(text=True)\n",
    "        visible_texts = filter(self.tag_visible, texts)  \n",
    "        return u\" \".join(t.strip() for t in visible_texts)\n",
    "    \n",
    "    def parse(self):\n",
    "        '''Extracts plain text, images and links from the document.\n",
    "        '''\n",
    "        self.soup = BeautifulSoup(self.content, 'html.parser')\n",
    "        self.anchors = self.get_anchors_from_html()\n",
    "        self.images = self.get_images_from_html()\n",
    "        self.text = self.get_text_from_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://infatica.io/blog/assets/images/Logo-black.svg?v=8fcb0d6019',\n",
       " 'https://infatica.io/blog/assets/images/blog/menu-1.svg?v=8fcb0d6019',\n",
       " 'https://infatica.io/blog/content/images/2020/06/mikayla-alston-portrait.jpg',\n",
       " 'https://infatica.io/blog/assets/images/Logo-white.svg?v=8fcb0d6019',\n",
       " 'https://infatica.io/blog/content/images/2021/07/web-scraping-legal-disclaimer.png',\n",
       " 'https://infatica.io/blog/assets/images/blog/menu-2.svg?v=8fcb0d6019',\n",
       " 'https://infatica.io/blog/content/images/2022/12/how-to-set-up-proxies-on-android.png',\n",
       " 'https://infatica.io/blog/content/images/2021/07/why-choose-python.png',\n",
       " 'https://infatica.io/blog/content/images/2021/07/using-python-to-scrape-images-from-the-web.png',\n",
       " 'https://infatica.io/blog/content/images/2023/01/web-crawlers-explained.png',\n",
       " 'https://infatica.io/blog/content/images/2023/01/http-proxies-explained.png',\n",
       " 'https://infatica.io/blog/content/images/2020/06/lucas-walker-portrait.jpg',\n",
       " 'https://infatica.io/blog/assets/images/blog/menu-4.svg?v=8fcb0d6019',\n",
       " 'https://infatica.io/blog/content/images/2021/07/scraping-images-with-proxies.png',\n",
       " 'https://infatica.io/blog/scrape-images-with-python/ /blog/assets/images/Finances-Online-Logo@2x.svg?v=8fcb0d6019',\n",
       " 'https://infatica.io/blog/content/images/2020/06/sharon-bennett-portrait-1.jpeg',\n",
       " 'https://infatica.io/blog/content/images/2021/07/inspect-image.png']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = HtmlDocument(\"https://infatica.io/blog/scrape-images-with-python/\")\n",
    "doc.get()\n",
    "doc.parse()\n",
    "# doc.text\n",
    "doc.images\n",
    "# doc.anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = HtmlDocument(\"http://sprotasov.ru\")\n",
    "doc.get()\n",
    "doc.parse()\n",
    "\n",
    "assert \"just few links\" in doc.text, \"Error parsing text\"\n",
    "assert \"http://sprotasov.ru/images/gb.svg\" in doc.images, \"Error parsing images\"\n",
    "assert any(p[1] == \"https://twitter.com/07C3\" for p in doc.anchors), \"Error parsing links\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. [10] Document analysis ##\n",
    "Complete the code for `HtmlDocumentTextData` class. Implement word and sentence splitting (use any method you can propose). \n",
    "\n",
    "**Criteria to succeed in the task**: \n",
    "1. Your `get_word_stats()` method should return `Counter` object.\n",
    "2. Don't forget to lowercase your words for counting.\n",
    "3. Sentences should be obtained from inside `<body>` tag only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "class HtmlDocumentTextData:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.doc = HtmlDocument(url)\n",
    "        self.doc.get()\n",
    "        self.doc.parse()\n",
    "    \n",
    "    def get_sentences(self) -> list:\n",
    "        '''Parses sentences.\n",
    "        '''\n",
    "        result = []\n",
    "\n",
    "        sents = sent_tokenize(self.doc.text)\n",
    "        for sent in sents:\n",
    "            sents_ = sent.split('  ')\n",
    "            for sent_ in sents_:\n",
    "                if sent_ != '':\n",
    "                    result.append(sent_)\n",
    "                    \n",
    "        return result\n",
    "    \n",
    "    def get_word_stats(self) -> Counter:\n",
    "        '''Returns Counter object of the document, \n",
    "        containing mapping {`word` -> count_in_doc}.\n",
    "        '''\n",
    "        counter = Counter()\n",
    "\n",
    "        for sent in self.get_sentences():\n",
    "            words = word_tokenize(sent)\n",
    "            for word in words:\n",
    "                if word not in punctuation:\n",
    "                    counter[word.lower()] += 1\n",
    "\n",
    "        return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('и', 40), ('в', 24), ('иннополис', 21), ('по', 14), ('университет', 12), ('на', 12), ('центр', 11), ('с', 11), ('университета', 10), ('для', 10)]\n"
     ]
    }
   ],
   "source": [
    "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
    "\n",
    "print(doc.get_word_stats().most_common(10))\n",
    "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис should be among most common'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. [15] Crawling ##\n",
    "\n",
    "Method `crawl_generator()` is given starting url (`source`) and max depth of search. It should return a **generator** of `HtmlDocumentTextData` objects (return a document as soon as it is downloaded and parsed). You can benefit from `yield obj_name` python construction. Use `HtmlDocumentTextData.anchors` field to go deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class Crawler:\n",
    "    \n",
    "    def crawl_generator(self, source, depth=1):\n",
    "        queues = [Queue() for _ in range(depth + 1)]\n",
    "        queues[0].put(source)\n",
    "        processed = set()\n",
    "\n",
    "        for i in range(depth + 1):\n",
    "            while not queues[i].empty():\n",
    "                url = queues[i].get()\n",
    "\n",
    "                if url in processed:\n",
    "                    continue\n",
    "                else:\n",
    "                    # clear_output(wait=True)\n",
    "                    processed.add(url)\n",
    "                    # print(len(processed))\n",
    "                    # print(url)\n",
    "\n",
    "                try:\n",
    "                    document = HtmlDocumentTextData(url)\n",
    "\n",
    "                    if i < depth:\n",
    "                        for _, anchor in document.doc.anchors:\n",
    "                            queues[i + 1].put(anchor)\n",
    "                            \n",
    "                    yield document\n",
    "                            \n",
    "                except FileNotFoundError:\n",
    "                    print(url, 'was not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://innopolis.university/en/\n",
      "341 distinct word(s) so far\n",
      "https://vk.com/innopolisu\n",
      "690 distinct word(s) so far\n",
      "https://innopolis.university/en/international-relations-office/\n",
      "1221 distinct word(s) so far\n",
      "https://innopolis.university/en/research/\n",
      "1300 distinct word(s) so far\n",
      "https://media.innopolis.university/en\n",
      "1360 distinct word(s) so far\n",
      "403 Forbidden for http://www.campuslife.innopolis.ru\n",
      "http://www.campuslife.innopolis.ru was not found.\n",
      "https://apply.innopolis.university/en/\n",
      "1846 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/master/\n",
      "1900 distinct word(s) so far\n",
      "https://career.innopolis.university/en/\n",
      "2049 distinct word(s) so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
      "Skipping https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
      "https://innopolis.university/en/team-structure/\n",
      "2055 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/bachelor/\n",
      "2147 distinct word(s) so far\n",
      "https://media.innopolis.university/news/innopolis-university-extends-international-application-deadline-/\n",
      "2302 distinct word(s) so far\n",
      "https://university.innopolis.ru/en/about/ was not found.\n",
      "https://minobrnauki.gov.ru/\n",
      "2614 distinct word(s) so far\n",
      "https://innopolis.university/en/form/\n",
      "2745 distinct word(s) so far\n",
      "https://media.innopolis.university/en/news/\n",
      "2745 distinct word(s) so far\n",
      "https://media.innopolis.university/news/registration-innopolis-open-2020/\n",
      "2854 distinct word(s) so far\n",
      "https://media.innopolis.university/news/self-driven-school/\n",
      "2960 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/postgraduate-study/\n",
      "3024 distinct word(s) so far\n",
      "404 Not Found for https://innopolis.university/en/university@innopolis.ru\n",
      "https://innopolis.university/en/university@innopolis.ru was not found.\n",
      "https://innopolis.university/en/proekty/activity/\n",
      "3083 distinct word(s) so far\n",
      "https://innopolis.university/en/about/\n",
      "3187 distinct word(s) so far\n",
      "https://career.innopolis.university/en/job/\n",
      "3641 distinct word(s) so far\n",
      "https://innopolis.university/en/team-structure/techcenters/\n",
      "3644 distinct word(s) so far\n",
      "https://media.innopolis.university/en/events/\n",
      "3647 distinct word(s) so far\n",
      "https://innopolis.university/en/organizatsiya-i-provedenie-meropriyatiy/\n",
      "3792 distinct word(s) so far\n",
      "https://innopolis.university/en/campus\n",
      "3880 distinct word(s) so far\n",
      "https://media.innopolis.university/news/webinar-interstudents-eng/\n",
      "3893 distinct word(s) so far\n",
      "https://panoroo.com/virtual-tours/NvQZM6B2\n",
      "3894 distinct word(s) so far\n",
      "https://career.innopolis.university/konkursnyezayavkiprofessorskoprepodavatelskogosostava/\n",
      "3999 distinct word(s) so far\n",
      "https://innopolis.university/en/contacts/\n",
      "4007 distinct word(s) so far\n",
      "https://media.innopolis.university/news/devops-summer-school/\n",
      "4122 distinct word(s) so far\n",
      "https://dovuz.innopolis.university/\n",
      "4284 distinct word(s) so far\n",
      "https://innopolis.university/en/startupstudio/\n",
      "4323 distinct word(s) so far\n",
      "https://apply.innopolis.university/en\n",
      "4323 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/stud-life/\n",
      "4387 distinct word(s) so far\n",
      "https://innopolis.university/lk/\n",
      "4591 distinct word(s) so far\n",
      "https://media.innopolis.university/news/webinar-for-international-candidates-/\n",
      "4597 distinct word(s) so far\n",
      "https://innopolis.university/en/faculty/\n",
      "5216 distinct word(s) so far\n",
      "https://www.youtube.com/user/InnopolisU\n",
      "5227 distinct word(s) so far\n",
      "https://innopolis.university/search/\n",
      "5228 distinct word(s) so far\n",
      "https://media.innopolis.university/en/\n",
      "5228 distinct word(s) so far\n",
      "https://t.me/universityinnopolis\n",
      "5235 distinct word(s) so far\n",
      "https://innopolis.university/en/nir2022/\n",
      "5406 distinct word(s) so far\n",
      "https://innopolis.university/en/writinghubhome/\n",
      "5419 distinct word(s) so far\n",
      "https://innopolis.university/en/team-structure/education-academics/\n",
      "5419 distinct word(s) so far\n",
      "https://innopolis.university/en/?special=Y\n",
      "5428 distinct word(s) so far\n",
      "404 Not Found for https://innopolis.university/proekty/activity/en\"\n",
      "https://innopolis.university/proekty/activity/en\" was not found.\n",
      "https://innopolis.university/en/teachingexcellencecenter/\n",
      "5612 distinct word(s) so far\n",
      "https://innopolis.university/en/team-structure/team-faculty/\n",
      "5625 distinct word(s) so far\n",
      "https://innopolis.university/\n",
      "5674 distinct word(s) so far\n",
      "https://innopolis.university/en/incomingstudents/\n",
      "6002 distinct word(s) so far\n",
      "https://innopolis.university/en/team/\n",
      "6002 distinct word(s) so far\n",
      "https://alumni.innopolis.university/\n",
      "6197 distinct word(s) so far\n",
      "https://innopolis.university/en/ido/\n",
      "6222 distinct word(s) so far\n",
      "https://innopolis.university/en/team-structure/team-faculty2/\n",
      "6232 distinct word(s) so far\n",
      "https://apply.innopolis.ru/en/\n",
      "7851 distinct word(s) so far\n",
      "https://innopolis.university/en/outgoingstudents/\n",
      "8131 distinct word(s) so far\n",
      "https://innopolis.university/en/internationalpartners/\n",
      "8203 distinct word(s) so far\n",
      "https://innopolis.university/en/board/\n",
      "8261 distinct word(s) so far\n",
      "http://www.minsvyaz.ru/en/ was not found.\n",
      "https://media.innopolis.university/news/cyber-resilience-petrenko/\n",
      "8396 distinct word(s) so far\n",
      "Done\n",
      "[('the', 2802), ('and', 2560), ('of', 2316), ('in', 1427), ('to', 1381), ('university', 1186), ('for', 862), ('a', 860), ('innopolis', 639), ('i', 576), ('at', 558), ('и', 539), ('is', 524), ('it', 494), ('в', 451), ('research', 428), ('•', 419), ('with', 406), ('education', 405), ('students', 400)]\n"
     ]
    }
   ],
   "source": [
    "crawler = Crawler()\n",
    "counter = Counter()\n",
    "\n",
    "for c in crawler.crawl_generator(\"https://innopolis.university/en/\", 1):\n",
    "    print(c.doc.url)\n",
    "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "        print(\"Skipping\", c.doc.url)\n",
    "        continue\n",
    "    counter.update(c.get_word_stats())\n",
    "    print(len(counter), \"distinct word(s) so far\")\n",
    "    \n",
    "print(\"Done\")\n",
    "\n",
    "print(counter.most_common(20))\n",
    "assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis sould be among most common'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "82ca00de6dfeb9e2eef59d83b518ae9b61e5781747466ffa06053c845d8d3ce9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
